{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Семинар 1 - Введение\n",
    "\n",
    "---\n",
    "\n",
    "## __1 Погрешности__\n",
    "\n",
    "* Неустранимые\n",
    "\n",
    "Обусловлены погрешностями входных данных. Практически не будем затрагивать в курсе.\n",
    "\n",
    "* Устранимые\n",
    "\n",
    "    * Округления - неточность вычислений с плавающей точкой\n",
    "    * Метода - будем уделять наибольшое внимание\n",
    "\n",
    "### __Пример__\n",
    "\n",
    "> Нужно численно рассчитать производную.\n",
    "\n",
    "Есть много способов это сделать. Сравним точность двух следующих вариантов:\n",
    "\n",
    "$$ f'(x) = \\frac{f(x+h) - f(x)} {h} $$\n",
    "\n",
    "$$ f'(x) = \\frac{f(x+h) - f(x-h)} {2h} $$\n",
    "\n",
    "Понять, какой метод точнее, можно подстановкой следующего разложения Тейлора в формулы для расчета производной.\n",
    "\n",
    "$$ f(x \\pm h) = f(x) \\pm \\frac{f'(x) h}{1!} + ... $$\n",
    "\n",
    "Для второй формулы при подстановке получается погрешность метода порядка $ h^2 $. А для первой - $ h $. Таким образом, второй способ расчета дает меньшую погрешность метода.\n",
    "\n",
    "$$ \\epsilon_m = \\frac{f'''(x)h^2}{6} = \\frac{Mh^2}{6} $$\n",
    "\n",
    "Но есть еще погрешности округления (вычислений):\n",
    "\n",
    "$$ \\epsilon_О = \\frac{\\epsilon_0}{h} \\, , $$\n",
    "\n",
    "где $ \\epsilon_0 $ - некоторая базовая погрешность при вычислении $ f(x) $.\n",
    "\n",
    "Тогда суммарная погрешность:\n",
    "\n",
    "$$ E(h) = \\frac{Mh^2}{6} + \\frac{\\epsilon_0}{h} $$\n",
    "\n",
    "Теперь можно найти оптимальный шаг, чтобы минимизировать суммарную погрешность:\n",
    "\n",
    "$$ h_{opt} =  \\sqrt[3]{\\frac{3\\epsilon_0}{M}} $$\n",
    "\n",
    "## __2 МНК__\n",
    "\n",
    "Метод неопределенных коэффициентов (МНК)\n",
    "\n",
    "Пусть дан набор точек:\n",
    "\n",
    "x    | 1   | 2    | 3    | 5   | 7\n",
    "---  |---  |---   |---   |---  |---\n",
    "f    | 0,5 | 0,25 | 0,25 | 0,2 | 0,2\n",
    "\n",
    "\n",
    "$ f'(3) = $ ???\n",
    "\n",
    "Решим через МНК:\n",
    "\n",
    "$$ f'(x) = \\alpha f(1) + \\beta f(2) + \\gamma f(3) + \\delta f(5) + \\epsilon f(7) = f'(x) + Mh^p $$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$ f'(x_0) = \\alpha f(x_0 - 2h) + \\beta f(x_0 - h) + \\gamma f(x_0) + \\delta f(x_0 + 2h) + \\epsilon f(x_0 + 4h) $$\n",
    "\n",
    "Получаем коэффициенты из разложения Тейлора:\n",
    "\n",
    "Производная | Коэффициенты\n",
    "--- | ---\n",
    "$ f(x_0) $ | $  \\quad  \\alpha + \\beta + \\gamma + \\delta + \\epsilon = 0 $\n",
    "$ f'(x_0) $ | $  \\quad  (-2\\alpha - \\beta + 2\\delta + 4\\epsilon) h = 1 $\n",
    "$ f''(x_0) $ | $  ... \\,\\, = 0 $\n",
    "$ ... $ | $ ... $\n",
    "\n",
    "Так же из этой системы виден порядок задачи (N - 1 = 4).\n",
    "\n",
    "Для производных другого порядка - размышления аналогичны.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Семинар 2 - Нормы в конечномерных пространствах. СЛАУ.\n",
    "\n",
    "---\n",
    "\n",
    "Рассматриваем уравнения вида:\n",
    "\n",
    "$$ A\\vec{x} = \\vec{f} $$\n",
    "\n",
    "Где $ \\vec{x} $ - искомое\n",
    "\n",
    "Нужно уметь оценивать погрешность расчетов. Для этого используют норму вектора:\n",
    "\n",
    "$$ ||\\vec{x}||_1 = \\max|x_k| $$\n",
    "$$ ||\\vec{x}||_2 = \\sum\\limits_{k = 1}^{n} |x_k| $$\n",
    "$$ ||\\vec{x}||_3 = \\sqrt{\\left( \\vec{x}, \\, \\vec{x} \\right)} $$\n",
    "\n",
    "$ ||\\vec{x}||_1 $ - Кубическая норма; $ ||\\vec{x}||_2 $ - Октаэдрическая норма; $ ||\\vec{x}||_3 $ - Евклидова норма.\n",
    "\n",
    "Норма для матрицы:\n",
    "\n",
    "$$ ||A||_1 = \\sup\\limits_{\\vec{x} \\neq 0}\\frac{||A\\vec{x}||}{||\\vec{x}||} $$\n",
    "\n",
    "Говорят, что норма $ \\vec{x} $ согласована с нормой $ A $, если: $ ||A\\vec{x}|| \\leq ||A||\\cdot||\\vec{x}|| $\n",
    "\n",
    "\n",
    "Распишем норму матрицы для $ ||\\vec{x}||_1 $, $ ||\\vec{x}||_2 $ и $ ||\\vec{x}||_3 $:\n",
    "\n",
    "$ ||A||_1 = \\max\\limits_{1 \\leq i \\leq n}\\left( \\sum\\limits_{j = 1}^{n}{a_{ij}} \\right) $ \\\n",
    "$ ||A||_1 = \\max\\limits_{1 \\leq j \\leq n}\\left( \\sum\\limits_{i = 1}^{n}{a_{ij}} \\right) $ \\\n",
    "$ ||A||_3 = \\sqrt{\\max\\limits_{1 \\leq i \\leq n}\\lambda^i \\left( A^{*} \\cdot A \\right)} $\n",
    "\n",
    "Таким образом, получили оценку для относительной погрешности решения СЛАУ:\n",
    "\n",
    "$$ \\frac{||\\vec{\\Delta x}||}{||\\vec{x}||} \\leq \\frac{\\mu}{1 - \\mu\\cdot\\frac{||\\Delta A||}{||A||}}\n",
    "\\left( \\frac{||\\Delta A||}{|| A ||} + \\frac{||\\Delta f||}{||f||} \\right) $$\n",
    "\n",
    "Где $ \\mu $ - число обусловленности: $ \\mu = ||A||\\cdot||A^{-1}|| $\n",
    "\n",
    "Хорошо обусловленная задача: $ 1 \\leq \\mu \\sim 100 $ \\\n",
    "Плохо обусловленная задача: $ \\mu \\gg 100 $\n",
    "\n",
    "В случае, когда $ \\Delta A = 0$, можно использовать более точную оценку числа обусловленности:\n",
    "\n",
    "$ \\nu = \\frac{||\\vec{f}||}{||\\vec{x}||} \\cdot ||A^{-1}|| $\n",
    "\n",
    "\n",
    "Для $||A||_3$ Так же есть своя оценка $ \\mu $:\n",
    "\n",
    "$ \\mu_3 = \\frac{\\max \\lambda \\left( A \\right)}{\\min \\lambda \\left( A \\right)} $\n",
    "\n",
    "На практике выбирают норму, которая дает желаемый результат (сходимость).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Семинар 3 - Методы вычислительной математики\n",
    "\n",
    "---\n",
    "\n",
    "## __1Прямые методы решения СЛАУ__\n",
    "\n",
    "Методы, которые дают только погрешность вычисления. Но, обычно, подобные методы имеют высокую ассимптотическую сложноть.\n",
    "\n",
    "Рассматриваем системы вида:\n",
    "\n",
    "$$ A\\vec{x} = \\vec{b} $$\n",
    "\n",
    "### 1.1 Метод Гаусса\n",
    "\n",
    "1) Прямой ход:\n",
    "\n",
    "Ну ты знаешь...\n",
    "\n",
    "2) Обратный ход:\n",
    "\n",
    "Ну ты знаешь...\n",
    "\n",
    "Так же применяют метод Гаусса с выбором главного элемента: Строки переставляются так, чтобы деление происходило на максимальный элемент матрицы. Это повышает точность расчетов (в general case).\n",
    "\n",
    "### 1.2 LU-разложение\n",
    "\n",
    "1) Представляем изначальную систему в виде:\n",
    "\n",
    "$$ LU\\vec{x} = \\vec{b} $$\n",
    "\n",
    "$ L $ - нижнетреугольная. $ U $ - верхнетреугольная.\n",
    "\n",
    "2) Потом решаем две системы:\n",
    "\n",
    "$$ L\\vec{y} = b $$\n",
    "\n",
    "$$ U\\vec{x} = y $$\n",
    "\n",
    "Данное разложение существует и единственно если, и только если все главные миноры матрицы $ A $ отличны от нуля.\n",
    "\n",
    "### 1.3 Метод Холецкого (метод главного корня)\n",
    "\n",
    "Метод применим если $ A $ - симметричная и положительно определенная (все главные миноры положительны).\n",
    "\n",
    "1) Тогда изначальная система представима в виде:\n",
    "\n",
    "$$ LL^T \\vec{x} = \\vec{b} $$\n",
    "\n",
    "$ L $ - нижнетреугольная.\n",
    "\n",
    "2) Решаем систему в таком виде:\n",
    "\n",
    "........\n",
    "\n",
    "## 2 Итерационные методы\n",
    "\n",
    "Всегда будет присутствовать погрешность метода. Но зато быстро.\n",
    "\n",
    "\n",
    "Преобразуем систему вида:\n",
    "\n",
    "$$ A\\vec{x} = \\vec{b} $$\n",
    "\n",
    "$$ A\\vec{x} + \\vec{x} = \\vec{b} + \\vec{x} $$\n",
    "\n",
    "$$ \\vec{x} = \\vec{x} - A\\vec{x} + \\vec{b} $$\n",
    "\n",
    "Общая формула для итерации:\n",
    "\n",
    "$$ \\vec{x}_k = B\\vec{x}_{k-1} + \\vec{F} $$\n",
    "\n",
    "Есть теоремы, которые позволяют определить, сходится ли итерационное решение (и с какой скоростью)\n",
    "\n",
    "#### __Theorem__\n",
    "\n",
    "Достаточное условие сходимости: $ || B || = q < 1 $. При этом: $ || x^{k} - x^* || \\le q^k || x^0 - x^* || $\n",
    "   \n",
    "#### __Theorem__\n",
    "\n",
    "Критерий: $ | \\lambda \\left( B \\right) | < 1 $\n",
    "\n",
    "---\n",
    "\n",
    "Короче почитай.....\n",
    "\n",
    "Уравнения вида: $ L\\vec{x} + D\\vec{x} + U\\vec{x} = \\vec{b} $ решают методами Якоби и Зейделя.\n",
    "\n",
    "\n",
    "### Метод Якоби:\n",
    "\n",
    "$$ L\\vec{x}_k + D\\vec{x}_{k+1} + U\\vec{x}_k = \\vec{b} $$\n",
    "\n",
    "### Метод Зейделя:\n",
    "\n",
    "$$ L\\vec{x}_{k+1} + D\\vec{x}_{k+1} + U\\vec{x}_k = \\vec{b} $$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
